{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "509579ac-6ada-4813-bf93-7fc71b828a0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Install required libraries for embedding generation and vector storage\n",
    "# sentence-transformers: Open-source embedding model (production choice)\n",
    "# pinecone: Vector database client for semantic search\n",
    "\n",
    "%pip install sentence-transformers -q\n",
    "%pip install pinecone -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c2e2ce-0ce7-4257-9e8c-7180d95dcd44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50ecbd10-00a9-4e21-a58a-2abf549af715",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure Azure Storage access\n",
    "storage_account = \"sradatalake\"\n",
    "storage_key = \"\"  # Get from Azure Portal -> Storage Accounts -> Access keys\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "    storage_key\n",
    ")\n",
    "\n",
    "print(\" Azure Storage authentication configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddf97f72-9152-41ff-baae-951c2ec0086a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load processed Wikipedia data\n",
    "storage_account = \"sradatalake\"\n",
    "\n",
    "print(\"üì• Loading Wikipedia articles...\")\n",
    "df = spark.read.parquet(f\"abfss://processed-data@{storage_account}.dfs.core.windows.net/wikipedia_1000/\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {df.count()} articles\")\n",
    "df.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0360fb97-2ba0-43a9-b0d9-8e5db3a34022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Initialize sentence transformer model for embedding generation\n",
    "# Model: all-MiniLM-L6-v2 (384 dimensions, optimized for semantic similarity)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import pandas as pd\n",
    "\n",
    "# Load pre-trained sentence transformer from HuggingFace\n",
    "print(\"üì• Loading sentence transformer model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\" Model loaded successfully!\")\n",
    "\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def generate_embeddings(texts: pd.Series) -> pd.Series:\n",
    "    \"\"\"Generate embeddings using sentence-transformers\"\"\"\n",
    "    # Truncate texts to 5000 characters\n",
    "    texts_list = [str(t)[:5000] for t in texts.tolist()]\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = model.encode(texts_list, show_progress_bar=False)\n",
    "    \n",
    "    # Convert to list format\n",
    "    return pd.Series([emb.tolist() for emb in embeddings])\n",
    "\n",
    "print(\" Embedding function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8449f56-f97a-4c37-ac49-bdc0e624c5c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6: Generate embeddings for all articles using distributed processing\n",
    "print(\"üîÑ Generating embeddings for all articles...\")\n",
    "print(\"‚è≥ This will take 10-20 minutes for 1000 articles...\")\n",
    "\n",
    "# Generate embeddings\n",
    "df_embedded = df.withColumn(\n",
    "    \"embedding\",\n",
    "    generate_embeddings(col(\"text_clean\"))\n",
    ")\n",
    "\n",
    "# Cache to avoid recomputation\n",
    "df_embedded.cache()\n",
    "\n",
    "print(\"\\n‚úÖ Embeddings generated!\")\n",
    "print(\"\\nüìä Sample results:\")\n",
    "df_embedded.select(\"title\", \"text_length\", \"embedding\").show(5, truncate=50)\n",
    "\n",
    "# Check embedding dimension\n",
    "sample_embedding = df_embedded.select(\"embedding\").first()[0]\n",
    "print(f\"\\nüìè Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"üìä Total articles with embeddings: {df_embedded.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64376399-1627-4f70-befa-c5f981674b9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "output_path = f\"abfss://embeddings@{storage_account}.dfs.core.windows.net/wikipedia_1000_embeddings/\"\n",
    "\n",
    "print(f\"üíæ Saving embeddings to Azure Storage...\")\n",
    "print(f\"üìç Location: {output_path}\")\n",
    "\n",
    "df_embedded.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully saved {df_embedded.count()} embeddings!\")\n",
    "\n",
    "# Verify save\n",
    "print(\"\\nüîç Verifying saved data...\")\n",
    "df_verify = spark.read.parquet(output_path)\n",
    "print(f\"‚úÖ Verification complete: {df_verify.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "120ea163-1476-40e0-ae64-98de202bef7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import time\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=\"pinecone-api-key\")\n",
    "\n",
    "# Index configuration\n",
    "index_name = \"wikipedia-search\"\n",
    "\n",
    "# Check if index exists\n",
    "existing_indexes = pc.list_indexes().names()\n",
    "\n",
    "if index_name in existing_indexes:\n",
    "    print(f\" Deleting existing index with wrong dimension...\")\n",
    "    pc.delete_index(index_name)\n",
    "    print(\" Old index deleted\")\n",
    "    time.sleep(5)  # Wait for deletion\n",
    "\n",
    "# Create new index with correct dimension\n",
    "print(f\"üìù Creating Pinecone index: {index_name}\")\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384,  # CORRECT dimension for Sentence Transformers all-MiniLM-L6-v2\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    )\n",
    ")\n",
    "print(f\" Index created with dimension 384!\")\n",
    "\n",
    "# Wait for index to be ready\n",
    "print(\"‚è≥ Waiting for index to initialize...\")\n",
    "while not pc.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)\n",
    "print(\" Index is ready!\")\n",
    "\n",
    "# Connect to index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Show index stats\n",
    "stats = index.describe_index_stats()\n",
    "print(f\"\\nüìä Pinecone Index Stats:\")\n",
    "print(f\"Dimension: 384\")\n",
    "print(f\"Total vectors: {stats.get('total_vector_count', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2137fae-566f-4aca-8e8f-885abe3d467c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 9: Upload Embeddings to Pinecone\n",
    "import re\n",
    "\n",
    "# Load embeddings from Azure\n",
    "print(\" Loading embeddings from Azure Storage...\")\n",
    "df_embedded = spark.read.parquet(f\"abfss://embeddings@{storage_account}.dfs.core.windows.net/wikipedia_1000_embeddings/\")\n",
    "print(f\" Loaded {df_embedded.count()} embeddings\")\n",
    "\n",
    "# Convert to Pandas\n",
    "print(\"\\nüì¶ Converting to Pandas for upload...\")\n",
    "df_pandas = df_embedded.select(\"title\", \"text_clean\", \"text_length\", \"embedding\").toPandas()\n",
    "print(f\" Converted {len(df_pandas)} records\")\n",
    "\n",
    "# Helper function to clean text\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove problematic Unicode characters\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = text.replace('\\u2013', '-')\n",
    "    text = text.replace('\\u2014', '-')\n",
    "    text = text.replace('\\u2018', \"'\")\n",
    "    text = text.replace('\\u2019', \"'\")\n",
    "    text = text.replace('\\u201c', '\"')\n",
    "    text = text.replace('\\u201d', '\"')\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    return text\n",
    "\n",
    "# Upload to Pinecone in batches\n",
    "print(f\"\\nüîÑ Uploading {len(df_pandas)} vectors to Pinecone...\")\n",
    "\n",
    "batch_size = 100\n",
    "vectors = []\n",
    "\n",
    "for idx, row in df_pandas.iterrows():\n",
    "    vector_id = f\"doc_{idx}\"\n",
    "    embedding = row['embedding']\n",
    "    \n",
    "    # Verify embedding dimension\n",
    "    if len(embedding) != 384:\n",
    "        print(f\"‚ö†Ô∏è Skipping vector {idx}: wrong dimension {len(embedding)}\")\n",
    "        continue\n",
    "    \n",
    "    # Clean text\n",
    "    title_clean = clean_text(row['title'])\n",
    "    text_clean = clean_text(row['text_clean'])\n",
    "    \n",
    "    metadata = {\n",
    "        \"title\": title_clean[:200],\n",
    "        \"text\": text_clean[:1000],\n",
    "        \"text_length\": int(row['text_length'])\n",
    "    }\n",
    "    \n",
    "    vectors.append({\n",
    "        \"id\": vector_id,\n",
    "        \"values\": embedding,\n",
    "        \"metadata\": metadata\n",
    "    })\n",
    "    \n",
    "    # Upload in batches\n",
    "    if len(vectors) >= batch_size:\n",
    "        try:\n",
    "            index.upsert(vectors=vectors)\n",
    "            print(f\"‚úÖ Uploaded {idx + 1}/{len(df_pandas)} vectors...\")\n",
    "            vectors = []\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error at index {idx}: {e}\")\n",
    "            vectors = []\n",
    "\n",
    "# Upload remaining vectors\n",
    "if vectors:\n",
    "    try:\n",
    "        index.upsert(vectors=vectors)\n",
    "        print(f\"‚úÖ Uploaded final batch\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error uploading final batch: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Upload complete!\")\n",
    "\n",
    "# Verify\n",
    "time.sleep(2)\n",
    "stats = index.describe_index_stats()\n",
    "print(f\"\\nüìä Final Pinecone Stats:\")\n",
    "print(f\"Total vectors in index: {stats['total_vector_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4066936f-b29b-4d3c-a78c-e2dcdaf35304",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 10: Validation query - Test semantic search functionality\n",
    "query_text = \"artificial intelligence and machine learning\"\n",
    "\n",
    "print(f\"üîç Searching for: '{query_text}'\")\n",
    "\n",
    "# Generate embedding for query (use same model)\n",
    "query_embedding = model.encode([query_text])[0].tolist()\n",
    "\n",
    "# Search Pinecone\n",
    "results = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=5,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Top 5 Most Similar Articles:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, match in enumerate(results['matches'], 1):\n",
    "    print(f\"\\n{i}. {match['metadata']['title']}\")\n",
    "    print(f\"   Similarity Score: {match['score']:.4f}\")\n",
    "    print(f\"   Text Length: {match['metadata']['text_length']} characters\")\n",
    "    print(f\"   Preview: {match['metadata']['text'][:300]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be75bf6-7797-4d2b-857b-b64a967cc1aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def search_wikipedia(query, top_k=5):\n",
    "    \"\"\"Search Wikipedia articles using semantic search\"\"\"\n",
    "    print(f\"üîç Searching for: '{query}'\")\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = model.encode([query])[0].tolist()\n",
    "    \n",
    "    # Search\n",
    "    results = index.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüìä Top {top_k} Results:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, match in enumerate(results['matches'], 1):\n",
    "        print(f\"\\n{i}. {match['metadata']['title']}\")\n",
    "        print(f\"   Score: {match['score']:.4f}\")\n",
    "        print(f\"   {match['metadata']['text'][:200]}...\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Try different searches\n",
    "\n",
    "search_wikipedia(\"climate change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73fb0620-e0c9-418a-9e15-a9edde9bec31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "08-Upload-Wikipedia-Pinecone",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
