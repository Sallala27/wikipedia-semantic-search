{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "150b2782-9cdc-4511-babf-ceaf35e703dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Install OpenAI SDK for embedding generation\n",
    "%pip install openai\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a4faba6-3ecb-4872-a36a-52e907d879ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Configure OpenAI embedding generation \n",
    "import openai\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import pandas as pd\n",
    "\n",
    "openai.api_key = \"\"  # Your OpenAI key\n",
    "\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def generate_embeddings(texts: pd.Series) -> pd.Series:\n",
    "    \"\"\"Generate embeddings for Wikipedia articles\"\"\"\n",
    "    embeddings = []\n",
    "    batch_size = 100\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size].tolist()\n",
    "        # Truncate to 8000 characters (model limit)\n",
    "        batch = [str(t)[:8000] for t in batch]\n",
    "        \n",
    "        try:\n",
    "            response = openai.Embedding.create(\n",
    "                input=batch,\n",
    "                model=\"text-embedding-3-small\"\n",
    "            )\n",
    "            batch_embeddings = [item['embedding'] for item in response['data']]\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            embeddings.extend([None] * len(batch))\n",
    "    \n",
    "    return pd.Series(embeddings)\n",
    "\n",
    "print(\" Embedding function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c3d718e-4f2c-4508-a12e-3dfd09575248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure Azure storage authentication\n",
    "# Cell 3: Load processed Wikipedia articles and generate sample embeddings\n",
    "storage_account = \"sradatalake\"\n",
    "\n",
    "storage_key = \"\"  # Azure storage key\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "    storage_key\n",
    ")\n",
    "\n",
    "# Now load the data\n",
    "df = spark.read.parquet(f\"abfss://processed-data@{storage_account}.dfs.core.windows.net/wikipedia_1000/\")\n",
    "print(f\" Loaded {df.count()} articles\")\n",
    "\n",
    "# Take first 100 for testing\n",
    "df_sample = df.limit(100)\n",
    "print(f\"üîÑ Generating embeddings for {df_sample.count()} articles...\")\n",
    "\n",
    "# Generate embeddings\n",
    "df_embedded = df_sample.withColumn(\n",
    "    \"embedding\",\n",
    "    generate_embeddings(col(\"text_clean\"))\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(\"\\nüìä Sample results:\")\n",
    "df_embedded.select(\"title\", \"text_length\", \"embedding\").show(5, truncate=50)\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {df_embedded.count()} embeddings\")\n",
    "\n",
    "# Check embedding dimensions\n",
    "sample_embedding = df_embedded.select(\"embedding\").first()[0]\n",
    "if sample_embedding:\n",
    "    print(f\"üìè Embedding dimension: {len(sample_embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "287cd9dd-da13-46e0-9f43-280a329040fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Persist sample embeddings to Azure Storage\n",
    "# Saves development/test embeddings for evaluationstorage_account = \"sradatalake\"\n",
    "\n",
    "output_path = f\"abfss://embeddings@{storage_account}.dfs.core.windows.net/wikipedia_100/\"\n",
    "\n",
    "print(f\"üíæ Saving embeddings to: {output_path}\")\n",
    "\n",
    "df_embedded.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "print(f\"‚úÖ Saved {df_embedded.count()} embeddings!\")\n",
    "print(f\"üìç Location: {output_path}\")\n",
    "\n",
    "# Verify the save\n",
    "print(\"\\nüîç Verifying saved data...\")\n",
    "df_verify = spark.read.parquet(output_path)\n",
    "print(f\"‚úÖ Verification successful! Loaded {df_verify.count()} records\")\n",
    "df_verify.select(\"title\", \"text_length\", \"embedding\").show(3, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad4559bf-40c8-4086-b457-7eb378efc57c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"sradatalake\"\n",
    "\n",
    "# Load all 1000 articles\n",
    "print(\"üì• Loading all Wikipedia articles...\")\n",
    "df = spark.read.parquet(f\"abfss://processed-data@{storage_account}.dfs.core.windows.net/wikipedia_1000/\")\n",
    "print(f\"‚úÖ Loaded {df.count()} articles\")\n",
    "\n",
    "# Generate embeddings for ALL articles\n",
    "print(f\"\\nüîÑ Generating embeddings for ALL {df.count()} articles...\")\n",
    "print(\"‚è≥ This will take 10-20 minutes...\")\n",
    "\n",
    "df_embedded = df.withColumn(\n",
    "    \"embedding\",\n",
    "    generate_embeddings(col(\"text_clean\"))\n",
    ")\n",
    "\n",
    "# Save\n",
    "output_path = f\"abfss://embeddings@{storage_account}.dfs.core.windows.net/wikipedia_1000_embeddings/\"\n",
    "print(f\"\\nüíæ Saving embeddings...\")\n",
    "\n",
    "df_embedded.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully saved {df_embedded.count()} embeddings!\")\n",
    "print(f\"üìç Location: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41cf65bf-b9bd-4a7d-bd4a-7ed299902961",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "07-Wikipedia-Embeddings",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
