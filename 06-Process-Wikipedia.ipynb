{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65d39c41-a19e-44da-9c75-3bbdae63430b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Cell 1: Install XML parsing library\n",
    "# lxml provides efficient XML processing for large Wikipedia dumps\n",
    "%pip install lxml\n",
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a36668e-d45f-47c2-94b8-760ca17a4617",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Configure Azure Data Lake Storage Gen2 authentication\n",
    "\n",
    "storage_account = \"sradatalake\"  \n",
    "storage_key = \"<your-storage-account-key\"  \n",
    "\n",
    "# Configure Spark to access Azure Storage directly\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "    storage_key\n",
    ")\n",
    "\n",
    "print(\" Storage configured! You can now access files directly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b752e00e-9dd1-46ab-9b26-c0b8eb9b6797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Verify Azure Storage connectivity and list source files\n",
    "# Displays file inventory with sizes for validation before processing\n",
    "\n",
    "container = \"pile-raw\"\n",
    "directory = \"dumps\"\n",
    "\n",
    "\n",
    "path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{directory}/\"\n",
    "\n",
    "files = dbutils.fs.ls(path)\n",
    "\n",
    "for f in files:\n",
    "    size_gb = f.size / (1024**3)\n",
    "    print(f\"{f.name}: {size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50c2bbcc-8844-4f7f-9960-f3466cf42aaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Initial data profiling - Extract sample articles for quality assessment\n",
    "# Processes 100K lines (~245 articles) to validate XML structure and content quality\n",
    "\n",
    "import bz2\n",
    "import xml.etree.ElementTree as ET\n",
    "from pyspark.sql import Row\n",
    "import io\n",
    "\n",
    "\n",
    "storage_account = \"sradatalake\"  \n",
    "file_path = f\"abfss://pile-raw@{storage_account}.dfs.core.windows.net/dumps/enwiki-latest-pages-articles.xml.bz2\"\n",
    "\n",
    "print(\" Reading file using Spark's native bz2 support...\")\n",
    "\n",
    "# Leverage Spark's distributed decompression for parallel processing\n",
    "text_lines = spark.read.text(file_path)\n",
    "\n",
    "print(\" File loaded into Spark DataFrame\")\n",
    "print(\" Processing lines to extract articles...\")\n",
    "\n",
    "# Sample size determined through initial data profiling\n",
    "sample_lines = text_lines.limit(100000).collect()\n",
    "\n",
    "print(f\"üìù Collected {len(sample_lines)} lines for parsing\")\n",
    "\n",
    "# Reconstruct full text from distributed line reads\n",
    "full_text = '\\n'.join([row.value for row in sample_lines])\n",
    "\n",
    "print(\" Parsing XML...\")\n",
    "\n",
    "# Parse the XML\n",
    "articles = []\n",
    "count = 0\n",
    "max_articles = 1000\n",
    "\n",
    "# XML namespace\n",
    "ns = {'mw': 'http://www.mediawiki.org/xml/export-0.10/'}\n",
    "\n",
    "try:\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    # Regex extraction optimized for large-scale XML processing\n",
    "    page_pattern = r'<page>(.*?)</page>'\n",
    "    pages = re.findall(page_pattern, full_text, re.DOTALL)\n",
    "    \n",
    "    print(f\"Found {len(pages)} pages in sample\")\n",
    "    \n",
    "    for page_text in pages:\n",
    "        if count >= max_articles:\n",
    "            break\n",
    "        \n",
    "        # Extract title\n",
    "        title_match = re.search(r'<title>(.*?)</title>', page_text)\n",
    "        # Extract text content\n",
    "        text_match = re.search(r'<text[^>]*>(.*?)</text>', page_text, re.DOTALL)\n",
    "        \n",
    "        if title_match and text_match:\n",
    "            title = title_match.group(1)\n",
    "            text = text_match.group(1)\n",
    "            \n",
    "            if text and len(text) > 100:\n",
    "                articles.append(Row(\n",
    "                    title=title,\n",
    "                    text=text,\n",
    "                    text_length=len(text)\n",
    "                ))\n",
    "                count += 1\n",
    "                \n",
    "                if count % 100 == 0:\n",
    "                    print(f\"Extracted {count} articles...\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Extracted {len(articles)} articles\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = spark.createDataFrame(articles)\n",
    "    df.show(5, truncate=50)\n",
    "    print(f\"\\nüìä Total articles: {df.count()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Parsing error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c07187-fb50-4f75-8be9-212bee2fac2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Full-scale extraction - Process target volume of articles\n",
    "# Increased to 500K lines based on profiling results (100K‚Üí245 articles; 500K‚Üí1000+ articles)\n",
    "\n",
    "import bz2\n",
    "import xml.etree.ElementTree as ET\n",
    "from pyspark.sql import Row\n",
    "import io\n",
    "import re\n",
    "\n",
    "storage_account = \"sradatalake\"\n",
    "file_path = f\"abfss://pile-raw@{storage_account}.dfs.core.windows.net/dumps/enwiki-latest-pages-articles.xml.bz2\"\n",
    "\n",
    "print(\" Reading file using Spark's native bz2 support...\")\n",
    "\n",
    "# Read as text lines (Spark handles bz2 decompression automatically)\n",
    "text_lines = spark.read.text(file_path)\n",
    "\n",
    "print(\" File loaded into Spark DataFrame\")\n",
    "print(\" Processing lines to extract articles...\")\n",
    "\n",
    "# Sample size optimized from initial profiling metrics\n",
    "sample_size = 500000\n",
    "sample_lines = text_lines.limit(sample_size).collect()\n",
    "\n",
    "print(f\" Collected {len(sample_lines)} lines for parsing\")\n",
    "\n",
    "# Join lines into text\n",
    "full_text = '\\n'.join([row.value for row in sample_lines])\n",
    "\n",
    "print(\" Parsing XML...\")\n",
    "\n",
    "articles = []\n",
    "count = 0\n",
    "max_articles = 1000\n",
    "\n",
    "# XML namespace\n",
    "ns = {'mw': 'http://www.mediawiki.org/xml/export-0.10/'}\n",
    "\n",
    "# Pattern to extract pages\n",
    "page_pattern = r'<page>(.*?)</page>'\n",
    "pages = re.findall(page_pattern, full_text, re.DOTALL)\n",
    "\n",
    "print(f\"Found {len(pages)} pages in sample\")\n",
    "\n",
    "for page_text in pages:\n",
    "    if count >= max_articles:\n",
    "        break\n",
    "    \n",
    "    # Extract title\n",
    "    title_match = re.search(r'<title>(.*?)</title>', page_text)\n",
    "    # Extract text content\n",
    "    text_match = re.search(r'<text[^>]*>(.*?)</text>', page_text, re.DOTALL)\n",
    "    \n",
    "    if title_match and text_match:\n",
    "        title = title_match.group(1)\n",
    "        text = text_match.group(1)\n",
    "        \n",
    "        if text and len(text) > 100:\n",
    "            articles.append(Row(\n",
    "                title=title,\n",
    "                text=text,\n",
    "                text_length=len(text)\n",
    "            ))\n",
    "            count += 1\n",
    "            \n",
    "            if count % 100 == 0:\n",
    "                print(f\"Extracted {count} articles...\")\n",
    "\n",
    "print(f\"\\n‚úÖ Extracted {len(articles)} articles\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(articles)\n",
    "df.show(5, truncate=50)\n",
    "\n",
    "print(f\"\\nüìä DataFrame Statistics:\")\n",
    "print(f\"Total articles: {df.count()}\")\n",
    "df.describe(['text_length']).show()\n",
    "\n",
    "# Display sample article for quality verification\n",
    "print(\"\\nüìñ Sample article:\")\n",
    "sample = df.filter(df.text_length > 1000).first()\n",
    "if sample:\n",
    "    print(f\"\\nTitle: {sample.title}\")\n",
    "    print(f\"Text preview: {sample.text[:500]}...\")\n",
    "    print(f\"Total length: {sample.text_length} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a02afb0-1cf8-434e-a06f-9a13d0569b9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6: Text cleaning and normalization\n",
    "# Removes Wikipedia markup syntax and applies content quality filters\n",
    "\n",
    "from pyspark.sql.functions import col, length, regexp_replace\n",
    "\n",
    "# Clean Wikipedia markup\n",
    "df_clean = df.select(\n",
    "    col(\"title\"),\n",
    "    regexp_replace(col(\"text\"), r'\\[\\[|\\]\\]|\\{\\{|\\}\\}|\\'\\'\\'|\\'\\'|==|<.*?>', ' ').alias(\"text_clean\"),\n",
    "    col(\"text_length\")\n",
    ").filter(\n",
    "    (col(\"text_length\") >= 500) &  # At least 500 characters\n",
    "    (col(\"text_length\") <= 50000)  # Max 50K\n",
    ")\n",
    "\n",
    "print(f\"Original: {df.count()} articles\")\n",
    "print(f\"After filtering: {df_clean.count()} articles\")\n",
    "\n",
    "df_clean.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8af28c43-f5cc-4000-b4f8-94c7f17c93cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 7: Persist cleaned data to Azure Data Lake Storage\n",
    "# Saves processed articles in Parquet format for efficient downstream consumption\n",
    "\n",
    "from pyspark.sql.functions import col, length, regexp_replace\n",
    "\n",
    "# Clean Wikipedia markup\n",
    "df_clean = df.select(\n",
    "    col(\"title\"),\n",
    "    regexp_replace(col(\"text\"), r'\\[\\[|\\]\\]|\\{\\{|\\}\\}|\\'\\'\\'|\\'\\'|==|<.*?>', ' ').alias(\"text_clean\"),\n",
    "    col(\"text_length\")\n",
    ").filter(\n",
    "    (col(\"text_length\") >= 500) &  # At least 500 characters\n",
    "    (col(\"text_length\") <= 50000)  # Max 50K\n",
    ")\n",
    "\n",
    "print(f\"Original: {df.count()} articles\")\n",
    "print(f\"After filtering: {df_clean.count()} articles\")\n",
    "df_clean.show(5, truncate=50)\n",
    "\n",
    "# Save to correct container name\n",
    "output_path = f\"abfss://processed-data@{storage_account}.dfs.core.windows.net/wikipedia_1000/\"\n",
    "\n",
    "print(f\"\\nüíæ Saving cleaned articles to: {output_path}\")\n",
    "\n",
    "df_clean.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "article_count = df_clean.count()\n",
    "\n",
    "print(f\"‚úÖ Saved {article_count} cleaned articles!\")\n",
    "print(f\"üìç Location: {output_path}\")\n",
    "\n",
    "# Verify the save\n",
    "print(\"\\nüîç Verifying saved data...\")\n",
    "df_verify = spark.read.parquet(output_path)\n",
    "print(f\"‚úÖ Verification successful! Loaded {df_verify.count()} articles\")\n",
    "df_verify.show(3, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb23718-95e7-46f1-b1b0-3fb2de56ac58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 8: Data quality metrics and summary statistics\n",
    "# Generate descriptive analytics for processed article corpus\n",
    "from pyspark.sql.functions import avg, min, max\n",
    "\n",
    "stats = df_clean.agg(\n",
    "    avg(\"text_length\").alias(\"avg_length\"),\n",
    "    min(\"text_length\").alias(\"min_length\"),\n",
    "    max(\"text_length\").alias(\"max_length\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"üìä Statistics:\")\n",
    "print(f\"  Average length: {stats.avg_length:.0f} characters\")\n",
    "print(f\"  Shortest: {stats.min_length} characters\")\n",
    "print(f\"  Longest: {stats.max_length} characters\")\n",
    "\n",
    "# Show longest articles\n",
    "print(\"\\nüìö Longest articles:\")\n",
    "df_clean.orderBy(col(\"text_length\").desc()).select(\"title\", \"text_length\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce88f796-4f5c-44c0-bcf5-abad02d31833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06-Process-Wikipedia",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
